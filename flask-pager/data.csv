index,prediction,sent,sub_index
0,EF,Non-autoregressive (NAR) neural machine translation is usually done via knowledge distillation from an autoregressive (AR) model.,1
1,App,"Under this framework, we leverage large monolingual corpora to improve the NAR model’s performance, with the goal of transferring the AR model’s generalization ability while preventing overfitting.",2
2,Res,"On top of a strong NAR baseline, our experimental results on the WMT14 En-De and WMT16 En-Ro news translation tasks confirm that monolingual data augmentation consistently improves the performance of the NAR model to approach the teacher AR model’s performance, yields comparable or better results than the best non-iterative NAR methods in the literature and helps reduce overfitting in the training process.",3
3,EF,Sequence-to-sequence (seq2seq) network is a well-established model for text summarization task.,1
4,EF,"It can learn to produce readable content; however, it falls short in effectively identifying key regions of the source.",2
5,App,"In this paper, we approach the content selection problem for clinical abstractive summarization by augmenting salient ontological terms into the summarizer.",3
6,Res,"Our experiments on two publicly available clinical data sets (107,372 reports of MIMIC-CXR, and 3,366 reports of OpenI) show that our model statistically significantly boosts state-of-the-art results in terms of ROUGE metrics (with improvements: 2.9% RG-1, 2.5% RG-2, 1.9% RG-L), in the healthcare domain where any range of improvement impacts patients’ welfare.",4
7,EF,It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation.,1
8,App,In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document.,2
9,App,We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce.,3
10,Res,Our human annotators found substantial amounts of hallucinated content in all model generated summaries.,4
11,Res,"However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans.",5
12,Res,"Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",6
13,EF,"Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront.",1
14,EF,"As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document.",2
15,EF,"When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient.",3
16,App,"In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models.",4
17,App,"We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes).",5
18,Res,"Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.",6
19,EF,The supervised training of high-capacity models on large datasets containing hundreds of thousands of document-summary pairs is critical to the recent success of deep learning techniques for abstractive summarization.,1
20,EF,"Unfortunately, in most domains (other than news) such training data is not available and cannot be easily sourced.",2
21,App,"In this paper we enable the use of supervised learning for the setting where there are only documents available (e.g., product or business reviews) without ground truth summaries.",3
22,App,"We create a synthetic dataset from a corpus of user reviews by sampling a review, pretending it is a summary, and generating noisy versions thereof which we treat as pseudo-review input.",4
23,App,We introduce several linguistically motivated noise generation functions and a summarization model which learns to denoise the input and generate the original review.,5
24,App,"At test time, the model accepts genuine reviews and generates a summary containing salient opinions, treating those that do not reach consensus as noise.",6
25,Res,Extensive automatic and human evaluation shows that our model brings substantial improvements over both abstractive and extractive baselines.,7
26,EF,"In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls.",1
27,EF,The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance.,2
28,EF,"However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables.",3
29,App,"In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency.",4
30,Res,"We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.",5
31,EF,"Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear.",1
32,EF,There is accumulating evidence that neural models do not learn systematically.,2
33,App,"We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour.",3
34,App,"We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying.",4
35,App,"As a case study, we perform a series of experiments in the setting of natural language inference (NLI).",5
36,Res,"We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",6
37,App,"We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events.",1
38,App,"To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events.",2
39,App,We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events.,3
40,App,"Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events.",4
41,Res,"Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected.",5
42,Res,"In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory.",6
43,Res,"Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932).",7
44,Res,Our findings highlight the potential of using NLP tools to study the traces of human cognition in language.,8
45,EF,A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence).,1
46,App,"Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent.",2
47,App,We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish.,3
48,Res,"Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences.",4
49,Res,"We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",5
50,EF,"Recent work has found evidence that natural languages are shaped by pressures for efficient communication ? e.g. the more contextually predictable a word is, the fewer speech sounds or syllables it has (Piantadosi et al. 2011).",1
51,EF,Research on the degree to which speech and language are shaped by pressures for effective communication ? robustness in the face of noise and uncertainty ? has been more equivocal.,2
52,App,We develop a measure of contextual confusability during word recognition based on psychoacoustic data.,3
53,Res,"Applying this measure to naturalistic speech corpora, we find evidence suggesting that speakers alter their productions to make contextually more confusable words easier to understand.",4
54,App,"We take up the scientific question of what determines the preferred order of adjectives in English, in phrases such as big blue box where multiple adjectives modify a following noun.",1
55,App,"We implement and test four quantitative theories, all of which are theoretically motivated in terms of efficiency in human language production and comprehension.",2
56,App,"The four theories we test are subjectivity (Scontras et al., 2017), information locality (Futrell, 2019), integration cost (Dyer, 2017), and information gain, which we introduce.",3
57,App,We evaluate theories based on their ability to predict orders of unseen adjectives in hand-parsed and automatically-parsed dependency treebanks.,4
58,Res,"We find that subjectivity, information locality, and information gain are all strong predictors, with some evidence for a two-factor account, where subjectivity and information gain reflect a factor involving semantics, and information locality reflects collocational preferences.",5
59,App,This paper discusses the importance of uncovering uncertainty in end-to-end dialog tasks and presents our experimental results on uncertainty classification on the processed Ubuntu Dialog Corpus.,1
60,Res,"We show that instead of retraining models for this specific purpose, we can capture the original retrieval model’s underlying confidence concerning the best prediction using trivial additional computation.",2
61,EF,"Being engaging, knowledgeable, and empathetic are all desirable general qualities in a conversational agent.",1
62,EF,Previous work has introduced tasks and datasets that aim to help agents to learn those qualities in isolation and gauge how well they can express them.,2
63,EF,"But rather than being specialized in one single quality, a good open-domain conversational agent should be able to seamlessly blend them all into one cohesive conversational flow.",3
64,App,"In this work, we investigate several ways to combine models trained towards isolated capabilities, ranging from simple model aggregation schemes that require minimal additional training, to various forms of multi-task training that encompass several skills at all training stages.",4
65,App,"We further propose a new dataset, BlendedSkillTalk, to analyze how these capabilities would mesh together in a natural conversation, and compare the performance of different architectures and training schemes.",5
66,Res,"Our experiments show that multi-tasking over several tasks that focus on particular capabilities results in better blended conversation performance compared to models trained on a single skill, and that both unified or two-stage approaches perform well if they are constructed to avoid unwanted bias in skill selection or are fine-tuned on our new task.",6
67,EF,Human conversations naturally evolve around related concepts and hop to distant concepts.,1
68,App,"This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows.",2
69,App,"By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations.",3
70,App,"The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses.",4
71,Res,"Experiments on Reddit conversations demonstrate ConceptFlow’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures.",5
72,Cont,All source codes of this work are available at https://github.com/thunlp/ConceptFlow.,6
73,EF,"Although deep learning models have brought tremendous advancements to the field of open-domain dialogue response generation, recent research results have revealed that the trained models have undesirable generation behaviors, such as malicious responses and generic (boring) responses.",1
74,App,"In this work, we propose a framework named “Negative Training” to minimize such behaviors.",2
75,App,"Given a trained model, the framework will first find generated samples that exhibit the undesirable behavior, and then use them to feed negative training signals for fine-tuning the model.",3
76,Res,"Our experiments show that negative training can significantly reduce the hit rate of malicious responses, or discourage frequent responses and improve response diversity.",4
77,EF,The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST).,1
78,EF,This information is typically represented as a semantic frame that captures the and slot-labels provided by the user.,2
79,App,"We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains.",3
80,App,"We propose a recursive, hierarchical frame-based representation and show how to learn it from data.",4
81,App,"We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template.",5
82,App,We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end.,6
83,Res,We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.,7
84,EF,We study the task of semantic parse correction with natural language feedback.,1
85,EF,"Given a natural language utterance, most semantic parsing systems pose the problem as one-shot translation where the utterance is mapped to a corresponding logical form.",2
86,App,"In this paper, we investigate a more interactive scenario where humans can further interact with the system by providing free-form natural language feedback to correct the system when it generates an inaccurate interpretation of an initial utterance.",3
87,App,"We focus on natural language to SQL systems and construct, SPLASH, a dataset of utterances, incorrect SQL interpretations and the corresponding natural language feedback.",4
88,Res,We compare various reference models for the correction task and show that incorporating such a rich form of feedback can significantly improve the overall semantic parsing accuracy while retaining the flexibility of natural language interaction.,5
89,Res,"While we estimated human correction accuracy is 81.5%, our best model achieves only 25.1%, which leaves a large gap for improvement in future research.",6
90,Cont,SPLASH is publicly available at https://aka.ms/Splash_dataset.,7
91,EF,We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications.,1
92,EF,"It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare.",2
93,EF,However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods.,3
94,App,"In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models.",4
95,App,Our proposed method can be used with any binary class calibration scheme and a neural network model.,5
96,Res,"Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements.",6
97,Res,"We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems.",7
98,Res,We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets.,8
99,Res,Our method improves the calibration and model performance on out-of-domain test scenarios as well.,9
100,EF,Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies.,1
101,EF,"Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical.",2
102,App,"To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance.",3
103,App,"Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary.",4
104,Res,"We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.",5
105,EF,Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text.,1
106,App,"In this paper, we allow model developers to specify these types of inductive biases as natural language explanations.",2
107,App,"We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input.",3
108,Res,"Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3?20x less labeled data and improves on the baseline by 3?10 F1 points with the same amount of labeled data.",4
109,EF,"Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks.",1
110,EF,"However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples.",2
111,EF,"In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected.",3
112,EF,"One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks.",4
113,App,"In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting.",5
114,Res,"Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.",6
115,EF,"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures.",1
116,App,"In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks.",2
117,App,"We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models.",3
118,Res,"We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks.",4
119,Res,"Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.",5
120,EF,Sequence labeling is a fundamental task for a range of natural language processing problems.,1
121,EF,"When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly.",2
122,EF,"In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible.",3
123,App,"In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data).",4
124,App,It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module.,5
125,App,"Finally, it leads to a model reflecting the agreement (consensus) among multiple sources.",6
126,App,We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation.,7
127,Res,Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.,8
128,Res,We also demonstrate that the method can apply to various tasks and cope with different encoders.,9
129,App,"This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix.",1
130,App,TMix creates a large amount of augmented training samples by interpolating text in hidden space.,2
131,App,"Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data.",3
132,Res,"By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks.",4
133,Res,The improvement is especially prominent when supervision is extremely limited.,5
134,Cont,We have publicly released our code at https://github.com/GT-SALT/MixText.,6
135,EF,Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters.,1
136,EF,"However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices.",2
137,App,"In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model.",3
138,App,"Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.",4
139,App,"Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.",5
140,App,"To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model.",6
141,App,"Then, we conduct knowledge transfer from this teacher to MobileBERT.",7
142,Res,Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks.,8
143,Res,"On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone.",9
144,Res,"On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",10
145,EF,"Language models that use additional latent structures (e.g., syntax trees, coreference chains, knowledge graph links) provide several advantages over traditional language models.",1
146,EF,"However, likelihood-based evaluation of these models is often intractable as it requires marginalizing over the latent space.",2
147,EF,Existing works avoid this issue by using importance sampling.,3
148,EF,"Although this approach has asymptotic guarantees, analysis is rarely conducted on the effect of decisions such as sample size and choice of proposal distribution on the reported estimates.",4
149,App,"In this paper, we carry out this analysis for three models: RNNG, EntityNLM, and KGLM.",5
150,App,"In addition, we elucidate subtle differences in how importance sampling is applied in these works that can have substantial effects on the final estimates, as well as provide theoretical results which reinforce the validity of this technique.",6
151,EF,Transfer learning has fundamentally changed the landscape of natural language processing (NLP).,1
152,EF,Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.,2
153,EF,"However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data.",3
154,App,"To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance.",4
155,App,"The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating.",5
156,Res,"Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI.",6
157,Res,"Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.",7
158,EF,Neural Network Language Models (NNLMs) generate probability distributions by applying a softmax function to a distance metric formed by taking the dot product of a prediction vector with all word vectors in a high-dimensional embedding space.,1
159,EF,The dot-product distance metric forms part of the inductive bias of NNLMs.,2
160,Res,"Although NNLMs optimize well with this inductive bias, we show that this results in a sub-optimal ordering of the embedding space that structurally impoverishes some words at the expense of others when assigning probability.",3
161,App,"We present numerical, theoretical and empirical analyses which show that words on the interior of the convex hull in the embedding space have their probability bounded by the probabilities of the words on the hull.",4
162,EF,"Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications.",1
163,EF,Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks.,2
164,EF,"However, there has been no attempt to exploit GNN to create taxonomies.",3
165,App,"In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task.",4
166,App,Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain.,5
167,App,We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction.,6
168,App,"Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training.",7
169,App,The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain.,8
170,Res,Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.,9
171,EF,Pretraining NLP models with variants of Masked Language Model (MLM) objectives has recently led to a significant improvements on many tasks.,1
172,App,This paper examines the benefits of pretrained models as a function of the number of training samples used in the downstream task.,2
173,Res,"On several text classification tasks, we show that as the number of training examples grow into the millions, the accuracy gap between finetuning BERT-based model and training vanilla LSTM from scratch narrows to within 1%.",3
174,Res,Our findings indicate that MLM-based models might reach a diminishing return point as the supervised data size increases significantly.,4
175,EF,Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI).,1
176,EF,"Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI.",2
177,EF,"However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary.",3
178,App,"We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary.",4
179,Res,"This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy.",5
180,App,"We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks.",6
181,Res,Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.,7
182,EF,Deep and large pre-trained language models are the state-of-the-art for various natural language processing tasks.,1
183,EF,"However, the huge size of these models could be a deterrent to using them in practice.",2
184,EF,Some recent works use knowledge distillation to compress these huge models into shallow ones.,3
185,App,In this work we study knowledge distillation with a focus on multilingual Named Entity Recognition (NER).,1
186,Res,"In particular, we study several distillation strategies and propose a stage-wise optimization scheme leveraging teacher internal representations, that is agnostic of teacher architecture, and show that it outperforms strategies employed in prior works.",2
187,App,"Additionally, we investigate the role of several factors like the amount of unlabeled data, annotation resources, model architecture and inference latency to name a few.",3
188,Res,We show that our approach leads to massive compression of teacher models like mBERT by upto 35x in terms of parameters and 51x in terms of latency for batch inference while retaining 95% of its F1-score for NER over 41 languages.,4
189,EF,Authorship attribution aims to identify the author of a text based on the stylometric analysis.,1
190,EF,"Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text’s style.",2
191,App,"In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model.",3
192,EF,An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated ? a decision that is key to the adversary interested in authorship attribution.,4
193,Res,We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87.,5
194,Res,"The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner.",6
195,Res,Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.,7
196,EF,Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications.,1
197,EF,"However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications.",2
198,App,"We propose a simple but effective method, DeeBERT, to accelerate BERT inference.",3
199,App,Our approach allows samples to exit earlier without passing through the entire model.,4
200,Res,Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality.,5
201,Res,Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy.,6
202,Cont,Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks.,7
203,Cont,Code is available at https://github.com/castorini/DeeBERT.,8
204,EF,"In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy.",1
205,EF,"Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model.",2
206,App,We first define the task as a sequence-to-sequence problem.,3
207,App,"Afterwards, we propose an auxiliary synthetic task of bottom-up-classification.",4
208,App,"Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy’s layers, and map them into the word vector space.",5
209,App,We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search.,6
210,Res,"Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy.",7
211,Res,"With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.",8
212,App,We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts.,1
213,App,"Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1).",2
214,App,"We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates.",3
215,Res,Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task.,4
216,App,We discuss areas for improvement and potential applications for text-only speech scoring.,5
217,EF,Representation learning is a critical ingredient for natural language processing systems.,1
218,EF,"Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.",2
219,EF,"For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity.",3
220,App,"We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.",4
221,App,"Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning.",5
222,App,"Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.",6
223,Res,We show that Specter outperforms a variety of competitive baselines on the benchmark.,7
224,App,"We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program.",1
225,Res,"By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques.",2
226,App,"We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases.",3
227,Res,"By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art.",4
228,Res,"Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",5
229,EF,"Open Information Extraction systems extract (“subject text”, “relation text”, “object text”) triples from raw text.",1
230,EF,"Some triples are textual versions of facts, i.e., non-canonicalized mentions of entities and relations.",2
231,App,"In this paper, we investigate whether it is possible to infer new facts directly from the open knowledge graph without any canonicalization or any supervision from curated knowledge.",3
232,App,"For this purpose, we propose the open link prediction task,i.e., predicting test facts by completing (“subject text”, “relation text”, ?) questions.",4
233,App,An evaluation in such a setup raises the question if a correct prediction is actually a new fact that was induced by reasoning over the open knowledge graph or if it can be trivially explained.,5
234,App,"For example, facts can appear in different paraphrased textual variants, which can lead to test leakage.",6
235,App,"To this end, we propose an evaluation protocol and a methodology for creating the open link prediction benchmark OlpBench.",7
236,App,We performed experiments with a prototypical knowledge graph embedding model for openlink prediction.,8
237,Res,"While the task is very challenging, our results suggests that it is possible to predict genuinely new facts, which can not be trivially explained.",9
238,EF,"In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them.",1
239,App,We argue that such data can prove as a testing ground for understanding how we reason about information.,2
240,App,"To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.",3
241,Res,"Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning.",4
242,Res,"Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.",5
243,EF,Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA).,1
244,EF,We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed.,2
245,App,"In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments.",3
246,App,"Specifically, we “occlude” the majority of a document’s text and add context-sensitive commands that reveal “glimpses” of the hidden text to a model.",4
247,App,"We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making.",5
248,App,We believe that this setting can contribute in scaling models to web-level QA scenarios.,6
249,EF,"Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets.",1
250,App,"We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage.",2
251,App,"We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus.",3
252,Res,"The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set.",4
253,Res,"This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.",5
254,EF,Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech.,1
255,EF,"One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames.",2
256,EF,The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks.,3
257,App,In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions.,4
258,App,We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task.,5
259,Res,"Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.",6
260,EF,"Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP.",1
261,EF,Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream.,2
262,EF,"While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication).",3
263,EF,"More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic.",4
264,App,"In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG).",5
265,App,MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.,6
266,App,It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities.,7
267,App,"In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis.",8
268,Res,Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet.,9
269,Res,"On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.",10
270,App,"We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers.",1
271,App,We propose a novel multimodal approach to real-time sequence labeling in speech.,2
272,App,"Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition.",3
273,Res,"Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data.",4
274,Res,The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.,5
275,App,This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture.,1
276,App,"We particularly focus on the scene context provided by the visual information, to ground the ASR.",2
277,App,We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer.,3
278,App,"Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions.",4
279,Res,"Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models.",5
280,Res,"Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models.",6
281,Res,"Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",7
282,EF,"End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation.",1
283,EF,"Their performance is often assumed to be superior, though in many conditions this is not yet the case.",2
284,Res,"We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines.",3
285,App,"Further, we introduce two methods to incorporate phone features into ST models.",4
286,Res,"We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work ? by up to 9 BLEU on our low-resource setting.",5
287,EF,"Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people.",1
288,EF,"Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication.",2
289,App,"Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality.",3
290,App,"We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier.",4
291,App,"We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",5
292,EF,"To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners.",1
293,EF,"Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014).",2
294,App,In this work we study large-scale architectures and datasets for this goal.,3
295,App,"We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components.",4
296,App,"To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019).",5
297,App,"Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits.",6
298,Res,"Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).",7
299,EF,"Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue.",1
300,EF,"There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation.",2
301,App,"Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them.",3
302,Res,"We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.",4
303,EF,The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue.,1
304,App,"We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn.",2
305,App,The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS).,3
306,App,We evaluate our models using offline experiments as well as human listening tests.,4
307,Res,We show that human listeners consider certain response timings to be more natural based on the dialogue context.,5
308,Cont,The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.,6
309,App,"We introduce dodecaDialogue: a set of 12 tasks that measures if a conversational agent can communicate engagingly with personality and empathy, ask questions, answer questions by utilizing knowledge resources, discuss topics and situations, and perceive and converse about images.",1
310,App,"By multi-tasking on such a broad large-scale set of data, we hope to both move towards and measure progress in producing a single unified agent that can perceive, reason and converse with humans in an open-domain setting.",2
311,Res,"We show that such multi-tasking improves over a BERT pre-trained baseline, largely due to multi-tasking with very large dialogue datasets in a similar domain, and that the multi-tasking in general provides gains to both text and image-based tasks using several metrics in both the fine-tune and task transfer settings.",3
312,Res,"We obtain state-of-the-art results on many of the tasks, providing a strong baseline for this challenge.",4
313,EF,"In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language.",1
314,EF,"In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions.",2
315,App,"In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation.",3
316,App,"The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse.",4
317,App,"The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages.",5
318,App,"Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.",6
319,EF,"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text.",1
320,EF,"Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task.",2
321,EF,"Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult.",3
322,App,"To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text.",4
323,Res,Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.,5
324,App,"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document.",1
325,EF,"While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling?a special case of infilling where text is predicted at the end of a document.",2
326,App,"In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling.",3
327,App,"To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked.",4
328,Res,"We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.",5
329,Res,"Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.",6
330,EF,"Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion.",1
331,EF,This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context.,2
332,EF,Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation.,3
333,App,"In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2.",4
334,Res,We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.,5
335,EF,"Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation.",1
336,EF,"Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply.",2
337,App,We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues.,3
338,App,"Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization.",4
339,Res,Extensive experiments demonstrate that the proposed method leads to improved performance.,5
340,EF,Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output.,1
341,App,We propose to extend this framework with a simple and effective post-generation ranking approach.,2
342,App,"Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output.",3
343,Res,"We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies.",4
344,Res,Experiments on two machine translation (MT) datasets show new state-of-art results.,5
345,Res,"We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",6
346,EF,Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN).,1
347,App,"In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones.",2
348,Res,We show that existing state-of-the-art agents do not generalize well.,3
349,App,"To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially.",4
350,App,A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps.,5
351,App,The learning process is composed of two phases.,6
352,App,"In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps.",7
353,App,"In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions.",8
354,App,We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability.,9
355,Res,"Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better.",10
356,Cont,The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.,11
357,App,"We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents.",1
358,App,We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments.,2
359,App,"We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space.",3
360,App,"The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation.",4
361,Res,"Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction.",5
362,Res,"Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively.",6
363,Res,"By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.",7
364,App,"We apply a generative segmental model of task structure, guided by narration, to action segmentation in video.",1
365,App,We focus on unsupervised and weakly-supervised settings where no action labels are known during training.,2
366,Res,"Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos.",3
367,Res,"Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.",4
368,EF,The Minecraft Collaborative Building Task is a two-player game in which an Architect (A) instructs a Builder (B) to construct a target structure in a simulated Blocks World Environment.,1
369,Res,"We define the subtask of predicting correct action sequences (block placements and removals) in a given game context, and show that capturing B’s past actions as well as B’s perspective leads to a significant improvement in performance on this challenging language understanding problem.",2
370,EF,Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph.,1
371,App,"Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture.",2
372,App,"The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation.",3
373,Res,"Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.",4
374,EF,Visual features are a promising signal for learning bootstrap textual models.,1
375,EF,"However, blackbox learning models make it difficult to isolate the specific contribution of visual components.",2
376,App,"In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal.",3
377,App,"By constructing simplified versions of the model, we isolate the core factors that yield the model’s strong performance.",4
378,Res,"Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better.",5
379,Res,We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning.,6
380,EF,Variational Autoencoder (VAE) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks.,1
381,EF,"However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as “posterior collapse”.",2
382,EF,Previous approaches consider the Kullback?Leibler divergence (KL) individual for each datapoint.,3
383,App,"We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL’s distribution positive.",4
384,App,"Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior’s parameters.",5
385,App,"Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently.",6
386,Res,We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE).,7
387,Res,"Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.",8
388,App,"We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline?random word embeddings?focusing on the impact of the training set size and the linguistic properties of the task.",1
389,Res,"Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks.",2
390,App,"Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.",3
391,App,We study the potential for interaction in natural language classification.,1
392,App,"We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions.",2
393,App,"At each turn, our system decides between asking the most informative question or making the final classification pre-diction.",3
394,App,"The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks.",4
395,App,"We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.",5
396,EF,Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications.,1
397,EF,"With a large KG, the embeddings consume a large amount of storage and memory.",2
398,EF,This is problematic and prohibits the deployment of these techniques in many real world settings.,3
399,App,"Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes.",4
400,App,The approach can be trained end-to-end with simple modifications to any existing KG embedding technique.,5
401,Res,We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance.,6
402,App,The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.,7
403,EF,This work revisits the task of training sequence tagging models with limited resources using transfer learning.,1
404,App,We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings.,2
405,App,"Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines.",3
406,Res,We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.,4
407,EF,Pretrained masked language models (MLMs) require finetuning for most NLP tasks.,1
408,App,"Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one.",2
409,Res,We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks.,3
410,Res,"By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation.",4
411,Res,"We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP).",5
412,App,"One can finetune MLMs to give scores without masking, enabling computation in a single inference pass.",6
413,App,"In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages.",7
414,Cont,We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.,8
415,EF,"Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE.",1
416,EF,"However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict.",2
417,App,"In this work, we propose a novel distance-based approach for knowledge graph link prediction.",3
418,App,"First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations.",4
419,App,"The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity.",5
420,App,"Second, the graph context is integrated into distance scoring functions directly.",6
421,App,"Specifically, graph context is explicitly modeled via two directed context representations.",7
422,App,"Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively.",8
423,App,"The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases.",9
424,Res,"Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",10
425,EF,Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability.,1
426,EF,"In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone.",2
427,EF,"When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications.",3
428,App,Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.,4
429,Res,We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives.,5
430,Res,"Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline.",6
431,Res,"We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline.",7
432,App,PosCal training can be easily extendable to any types of classification tasks as a form of regularization term.,8
433,App,"Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.",9
434,EF,Text generation often requires high-precision output that obeys task-specific rules.,1
435,EF,This fine-grained control is difficult to enforce with off-the-shelf deep learning models.,2
436,App,"In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach.",3
437,App,"Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model.",4
438,App,"This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models.",5
439,App,Experiments consider applications of this approach for text generation.,6
440,Res,"We find that this method improves over standard benchmarks, while also providing fine-grained control.",7
441,EF,"Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions?",1
442,App,We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts.,2
443,Res,"We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller.",3
444,Res,"Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance.",4
445,Res,"We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness.",5
446,Res,"Finally, we show where future work can improve OOD robustness.",6
447,EF,"Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs.",1
448,EF,"Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT.",2
449,App,"In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.",3
450,App,"The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings.",4
451,App,"Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks.",5
452,App,"We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity).",6
453,App,We instantiate RobEn to defend against a large family of adversarial typos.,7
454,Res,"Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.",8
455,EF,"In natural language processing, a recently popular line of work explores how to best report the experimental results of neural networks.",1
456,EF,"One exemplar publication, titled “Show Your Work: Improved Reporting of Experimental Results” (Dodge et al., 2019), advocates for reporting the expected validation effectiveness of the best-tuned model, with respect to the computational budget.",2
457,App,"In the present work, we critically examine this paper.",3
458,Res,"As far as statistical generalizability is concerned, we find unspoken pitfalls and caveats with this approach.",4
459,Res,We analytically show that their estimator is biased and uses error-prone assumptions.,5
460,Res,We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals.,6
461,App,We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation.,7
462,Cont,Our codebase is athttps://github.com/castorini/meanmax.,8
463,EF,"BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA).",1
464,EF,BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction.,2
465,App,In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding.,3
466,App,"Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model’s parameters, it is selected from a relevant passage.",4
467,Res,We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets.,5
468,Res,"Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction.",6
469,Res,"We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system.",7
470,Res,"Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.",8
471,EF,Sentence ordering is the task of arranging the sentences of a given text in the correct order.,1
472,EF,Recent work using deep neural networks for this task has framed it as a sequence prediction problem.,2
473,App,"In this paper, we propose a new framing of this task as a constraint solving problem and introduce a new technique to solve it.",3
474,App,"Additionally, we propose a human evaluation for this task.",4
475,Res,The results on both automatic and human metrics across four different datasets show that this new technique is better at capturing coherence in documents.,5
476,EF,"Recently, NLP has seen a surge in the usage of large pre-trained models.",1
477,EF,"Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice.",2
478,EF,This raises the question of whether downloading untrusted pre-trained weights can pose a security threat.,3
479,App,"In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword.",4
480,App,"We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure.",5
481,Res,"Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat.",6
482,App,"Finally, we outline practical defenses against such attacks.",7
483,EF,Transformers have gradually become a key component for many state-of-the-art natural language representation models.,1
484,EF,"A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0.",2
485,EF,This model however is computationally prohibitive and has a huge number of parameters.,3
486,App,In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model.,4
487,App,We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency.,5
488,Res,We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers.,6
489,Res,"In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.",7
490,App,We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model.,1
491,App,"In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy.",2
492,App,This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model.,3
493,Res,"Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.",4
494,EF,Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged.,1
495,EF,The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT.,2
496,EF,"The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data.",3
497,App,"In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT.",4
498,Res,We offer three major results:,5
499,Res,(i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models.,6
500,Res,(ii) Self-supervision improves zero-shot translation quality in multilingual models.,7
501,Res,"(iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.",8
502,EF,Back-translation is a widely used data augmentation technique which leverages target monolingual data.,1
503,EF,"However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese.",2
504,EF,This is believed to be due to translationese inputs better matching the back-translated training data.,3
505,App,"In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators.",4
506,App,We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs.,5
507,App,BLEU cannot capture human preferences because references are translationese when source sentences are natural text.,6
508,Cont,We recommend complementing BLEU with a language model score to measure fluency.,7
509,EF,"Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information.",1
510,EF,"But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies.",2
511,App,We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies.,3
512,Res,"Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.",4
513,EF,Neural architectures are the current state of the art in Word Sense Disambiguation (WSD).,1
514,EF,"However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB).",2
515,App,"We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set.",3
516,Res,"As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks.",4
517,App,"On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.",5
518,EF,Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus.,1
519,App,"We show that characters’ written form, Glyphs, in ideographic languages could carry rich semantics.",2
520,App,"We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem.",3
521,App,"Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios.",4
522,Res,Experiments across different applications show the significant effectiveness of our model.,5
523,App,We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures.,1
524,App,"Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together.",2
525,App,"The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure.",3
526,App,We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity.,4
527,Res,"The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.",5
528,EF,"While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied.",1
529,App,"We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction.",2
530,App,"When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings.",3
531,App,"We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally.",4
532,Res,"Both models outperform previous approaches, with the multi-channel model performing best.",5
533,EF,Metaphor is a linguistic device in which a concept is expressed by mentioning another.,1
534,EF,"Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics.",2
535,EF,"Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models.",3
536,App,This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs.,4
537,App,"To the best of our knowledge, this is the first “MWE-aware” metaphor identification system paving the way for further experiments on the complex interactions of these phenomena.",5
538,Res,The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.,6
539,EF,Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language.,1
540,EF,"These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language.",2
541,EF,"While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages.",3
542,App,"In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications.",4
543,App,We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives.,5
544,Res,Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning.,6
545,Cont,We further provide recommendations for using the multilingual word representations for downstream tasks.,7
546,EF,"As part of growing NLP capabilities, coupled with an awareness of the ethical dimensions of research, questions have been raised about whether particular datasets and tasks should be deemed off-limits for NLP research.",1
547,App,"We examine this question with respect to a paper on automatic legal sentencing from EMNLP 2019 which was a source of some debate, in asking whether the paper should have been allowed to be published, who should have been charged with making such a decision, and on what basis.",2
548,App,"We focus in particular on the role of data statements in ethically assessing research, but also discuss the topic of dual use, and examine the outcomes of similar debates in other scientific disciplines.",3
549,EF,"Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity).",1
550,EF,"However, manually annotating a large dataset with a protected attribute is slow and expensive.",2
551,EF,"Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias?",3
552,EF,"While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias.",4
553,App,"In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval.",5
554,Res,We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias.,6
555,Cont,"In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim.",7
556,Res,Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases.,8
557,App,"For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences ? to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",9
558,EF,"Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.).",1
559,App,"We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data.",2
560,EF,Advanced machine learning techniques have boosted the performance of natural language processing.,1
561,EF,"Nevertheless, recent studies, e.g., (CITATION) show that these techniques inadvertently capture the societal bias hidden in the corpus and further amplify it.",2
562,EF,"However, their analysis is conducted only on models’ top predictions.",3
563,App,"In this paper, we investigate the gender bias amplification issue from the distribution perspective and demonstrate that the bias is amplified in the view of predicted probability distribution over labels.",4
564,App,We further propose a bias mitigation approach based on posterior regularization.,5
565,App,"With little performance loss, our method can almost remove the bias amplification in the distribution.",6
566,App,Our study sheds the light on understanding the bias amplification.,7
567,EF,Recent developments in Neural Relation Extraction (NRE) have made significant strides towards Automated Knowledge Base Construction.,1
568,EF,"While much attention has been dedicated towards improvements in accuracy, there have been no attempts in the literature to evaluate social biases exhibited in NRE systems.",2
569,App,"In this paper, we create WikiGenderBias, a distantly supervised dataset composed of over 45,000 sentences including a 10% human annotated test set for the purpose of analyzing gender bias in relation extraction systems.",3
570,Res,"We find that when extracting spouse-of and hypernym (i.e., occupation) relations, an NRE system performs differently when the gender of the target entity is different.",4
571,Res,"However, such disparity does not appear when extracting relations such as birthDate or birthPlace.",5
572,App,"We also analyze how existing bias mitigation techniques, such as name anonymization, word embedding debiasing, and data augmentation affect the NRE system in terms of maintaining the test performance and reducing biases.",6
573,Res,"Unfortunately, due to NRE models rely heavily on surface level cues, we find that existing bias mitigation approaches have a negative effect on NRE.",7
574,Cont,Our analysis lays groundwork for future quantifying and mitigating bias in NRE.,8
575,App,We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents.,1
576,App,We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance.,2
577,App,"Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing.",3
578,App,"Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures.",4
579,Res,We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.,5
580,EF,Pooling is an important technique for learning text representations in many neural NLP models.,1
581,EF,"In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features.",2
582,EF,"However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks.",3
583,EF,"In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited.",4
584,App,"In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation.",5
585,App,"Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks.",6
586,App,"In addition, we propose two methods to ensure the numerical stability of the model training.",7
587,App,"The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion.",8
588,App,"The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation.",9
589,Res,Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.,10
590,EF,Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks.,1
591,EF,"However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach.",2
592,App,We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups.,3
593,App,"Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel.",4
594,App,"Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work.",5
595,Res,We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance.,6
596,Cont,"We provide an efficient, open-source implementation.",7
597,EF,Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words.,1
598,EF,"However, the underlying reasons for their strong performance have not been well explained.",2
599,App,"In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax.",3
600,Res,"Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs.",4
601,Res,"Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling.",5
602,Res,"Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.",6
603,EF,Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers.,1
604,App,Could ordering the sublayers in a different pattern lead to better performance?,2
605,App,We generate randomly ordered transformers and train them with the language modeling objective.,3
606,Res,"We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top.",4
607,Res,"We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.",5
608,Res,"However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models.",6
609,Cont,"Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",7
610,EF,"Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort.",1
611,App,"In this study, we propose a novel method that replicates the effects of a model ensemble with a single model.",2
612,App,Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors.,3
613,Res,Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.,4
614,EF,"Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity.",1
615,EF,"In this situation, transferring from seen classes to unseen classes is extremely hard.",2
616,App,"To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data.",3
617,EF,"Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets.",4
618,App,We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection.,5
619,Res,Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification,6
620,EF,Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.,1
621,EF,"However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.",2
622,App,"To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.",3
623,App,"Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).",4
624,App,We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.,5
625,App,"Finally, these representations provide an attention-based context vector for the decoder.",6
626,App,We evaluate our proposed encoder on the Multi30K datasets.,7
627,Res,Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.,8
628,EF,Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest.,1
629,EF,One of the crucial parts in methods for the BLI task is the matching procedure.,2
630,EF,Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings.,3
631,App,Thus We propose a relaxed matching procedure to find a more precise matching between two languages.,4
632,Res,We also find that aligning source and target language embedding space bidirectionally will bring significant improvement.,5
633,App,We follow the previous iterative framework to conduct experiments.,6
634,Res,"Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.",7
635,App,"This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units.",1
636,App,We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference.,2
637,App,"A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability.",3
638,App,DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming.,4
639,Res,Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences.,5
640,Res,"DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).",6
641,App,We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages.,1
642,App,Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices.,2
643,App,This viewpoint arises from the aim to align the second order information of the two language spaces.,3
644,App,The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation.,4
645,Res,"Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs.",5
646,Res,The performance improvement is more significant for distant language pairs.,6
647,EF,Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process.,1
648,EF,"However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing.",2
649,App,"To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments.",3
650,App,The segments are generated simultaneously while each segment is predicted token-by-token.,4
651,App,"By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors.",5
652,Res,Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.,6
653,EF,"Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output.",1
654,Res,"While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference.",2
655,App,"By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models.",3
656,App,"Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.",4
657,App,"We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task.",1
658,App,A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation.,2
659,App,A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation.,3
660,Res,"The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task.",4
661,Cont,"To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.",5
662,EF,"Legal Judgement Prediction (LJP) is the task of automatically predicting a law case’s judgment results given a text describing the case’s facts, which has great prospects in judicial assistance systems and handy services for the public.",1
663,EF,"In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged.",2
664,EF,"To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems.",3
665,App,"In this paper, we present an end-to-end model, LADAN, to solve the task of LJP.",4
666,App,"To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions.",5
667,Res,Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.,6
668,EF,"Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think.",1
669,EF,"It is challenging to specify the level of education, experience, relevant skills per the company information and job description.",2
670,App,"To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions.",3
671,App,"To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA.",4
672,App,"Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels.",5
673,App,"At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results.",6
674,App,The proposed approach is evaluated on real-world job posting data.,7
675,Res,Experimental results clearly demonstrate the effectiveness of the proposed method.,8
676,EF,"The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code.",1
677,EF,ICD coding aims to assign proper ICD codes to a medical record.,2
678,EF,"Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task.",3
679,EF,"However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence.",4
680,App,"In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem.",5
681,App,"Specifically, we propose a hyperbolic representation method to leverage the code hierarchy.",6
682,App,"Moreover, we propose a graph convolutional network to utilize the code co-occurrence.",7
683,Res,Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.,8
684,EF,"Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling).",1
685,EF,Such operations limit the performance.,2
686,EF,"For instance, a multi-label document may contain several concepts.",3
687,EF,"In this case, one vector can not sufficiently capture its salient and discriminative content.",4
688,App,"Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits.",5
689,App,"First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents.",6
690,App,"Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks.",7
691,App,"To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing.",8
692,App,Extensive experiments are conducted on four benchmark datasets.,9
693,Res,"Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.",10
694,EF,"They typically contain user descriptions of the problem, the setup, and steps for attempted resolution.",1
695,EF,"Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces.",2
696,EF,These elements contain potentially crucial information for problem resolution.,3
697,EF,"However, they cannot be correctly parsed by tools designed for natural language.",4
698,App,"In this paper, we address the problem of segmentation for technical support questions.",5
699,App,"We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches.",6
700,App,"We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach.",7
701,Res,"We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model.",8
702,Res,"Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.",9
703,EF,"The prosperity of Massive Open Online Courses (MOOCs) provides fodder for many NLP and AI research for education applications, e.g., course concept extraction, prerequisite relation discovery, etc.",1
704,EF,"However, the publicly available datasets of MOOC are limited in size with few types of data, which hinders advanced models and novel attempts in related topics.",2
705,App,"Therefore, we present MOOCCube, a large-scale data repository of over 700 MOOC courses, 100k concepts, 8 million student behaviors with an external resource.",3
706,App,"Moreover, we conduct a prerequisite discovery task as an example application to show the potential of MOOCCube in facilitating relevant research.",4
707,Cont,The data repository is now available at http://moocdata.cn/data/MOOCCube.,5
708,EF,The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability.,1
709,App,"In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system.",2
710,App,"The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare.",3
711,Res,"The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.",4
712,EF,News editorials argue about political issues in order to challenge or reinforce the stance of readers with different ideologies.,1
713,EF,Previous research has investigated such persuasive effects for argumentative content.,2
714,App,"In contrast, this paper studies how important the style of news editorials is to achieve persuasion.",3
715,App,"To this end, we first compare content- and style-oriented classifiers on editorials from the liberal NYTimes with ideology-specific effect annotations.",4
716,Res,"We find that conservative readers are resistant to NYTimes style, but on liberals, style even has more impact than content.",5
717,App,"Focusing on liberals, we then cluster the leads, bodies, and endings of editorials, in order to learn about writing style patterns of effective argumentation",6
718,EF,"In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis.",1
719,EF,It aims at extracting the potential pairs of emotions and their corresponding causes in a document.,2
720,EF,"To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes.",3
721,EF,"However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step.",4
722,App,"To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme.",5
723,App,"A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs.",6
724,App,"The 2D representation, interaction, and prediction are integrated into a joint framework.",7
725,Res,"In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.",8
726,EF,Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document.,1
727,EF,"Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs.",2
728,EF,"However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well.",3
729,App,"In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction.",4
730,App,"It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking.",5
731,Res,"Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.",6
732,App,We present a simple but effective method for aspect identification in sentiment analysis.,1
733,App,"Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages.",2
734,App,"We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable.",3
735,EF,Previous work relied on syntactic features and complex neural models.,4
736,Res,"We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed.",5
737,Cont,The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.,6
738,EF,"Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target.",1
739,EF,Remarkable success has been achieved when sufficient labeled training data is available.,2
740,EF,"However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets.",3
741,App,"In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets.",4
742,App,"Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags.",5
743,App,"Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell.",6
744,Res,Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.,7
745,EF,"Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis.",1
746,App,"In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge.",2
747,App,"We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts.",3
748,App,These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner.,4
749,Res,"Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.",5
750,EF,"The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification.",1
751,EF,"Rather than considering the tasks separately, we build an end-to-end ABSA solution.",2
752,EF,Previous works in ABSA tasks did not fully leverage the importance of syntactical information.,3
753,EF,"Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms.",4
754,EF,"On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words.",5
755,App,This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning.,6
756,App,"We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor.",7
757,App,"We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms.",8
758,Res,This increases the accuracy of the aspect sentiment classifier.,9
759,Res,Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.,10
760,EF,The main barrier to progress in the task of Formality Style Transfer is the inadequacy of training data.,1
761,App,"In this paper, we study how to augment parallel data and propose novel and simple data augmentation methods for this task to obtain useful sentence pairs with easily accessible models and systems.",2
762,Res,"Experiments demonstrate that our augmented parallel data largely helps improve formality style transfer when it is used to pre-train the model, leading to the state-of-the-art results in the GYAFC benchmark dataset.",3
763,EF,Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews.,1
764,EF,Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words.,2
765,EF,"However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections.",3
766,App,"In this paper, we address this problem by means of effective encoding of syntax information.",4
767,App,"Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree.",5
768,App,"Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction.",6
769,Res,"Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.",7
770,EF,Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA).,1
771,EF,The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems.,2
772,EF,"However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms.",3
773,EF,"Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs.",4
774,App,"To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE).",5
775,App,"Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works.",6
776,App,"We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries.",7
777,App,"Meanwhile, the pair-wise relations are jointly identified using the span representations.",8
778,Res,Extensive experiments show that our model consistently outperforms state-of-the-art methods.,9
779,EF,Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”.,1
780,EF,"Due to the scarcity of labeled data, ORL remains challenging for data-driven methods.",2
781,App,"In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations.",3
782,App,We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels.,4
783,App,"In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously.",5
784,App,We verify our methods on the benchmark MPQA corpus.,6
785,Res,"The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline.",7
786,Res,"In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT).",8
787,Res,Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.,9
788,EF,State-of-the-art argument mining studies have advanced the techniques for predicting argument structures.,1
789,EF,"However, the technology for capturing non-tree-structured arguments is still in its infancy.",2
790,App,"In this paper, we focus on non-tree argument mining with a neural model.",3
791,App,We jointly predict proposition types and edges between propositions.,4
792,App,Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges.,5
793,Res,Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.,6
794,App,"We propose a novel linearization of a constituent tree, together with a new locally normalized model.",1
795,App,"For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them.",2
796,App,"Compared with global models, our model is fast and parallelizable.",3
797,Res,"Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective.",4
798,Res,Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.,5
799,EF,Unsupervised constituency parsing aims to learn a constituency parser from a training corpus without parse tree annotations.,1
800,EF,"While many methods have been proposed to tackle the problem, including statistical and neural methods, their experimental results are often not directly comparable due to discrepancies in datasets, data preprocessing, lexicalization, and evaluation metrics.",2
801,App,"In this paper, we first examine experimental settings used in previous work and propose to standardize the settings for better comparability between methods.",3
802,App,"We then empirically compare several existing methods, including decade-old and newly proposed ones, under the standardized settings on English and Japanese, two languages with different branching tendencies.",4
803,Res,We find that recent models do not show a clear advantage over decade-old models in our experiments.,5
804,App,We hope our work can provide new insights into existing methods and facilitate future empirical evaluation of unsupervised constituency parsing.,6
805,App,We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks.,1
806,App,"Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span.",2
807,App,Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference.,3
808,Res,"The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity.",4
809,Res,"Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.",5
810,EF,"In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation.",1
811,EF,"As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss.",2
812,Cont,This paper for the first time presents a second-order TreeCRF extension to the biaffine parser.,3
813,EF,"For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF.",4
814,App,"To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.",5
815,Res,"Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data.",6
816,Cont,We release our code athttps://github.com/yzhangcs/crfpar.,7
817,EF,"Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks.",1
818,EF,"Such tree-based networks can be provided with a constituency parse, a dependency parse, or both.",2
819,App,We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task.,3
820,Res,"We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement.",4
821,Res,"Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.",5
822,EF,Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages.,1
823,EF,"Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages.",2
824,EF,"However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations.",3
825,App,"In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student).",4
826,App,"We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student’s and the teachers’ structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions.",5
827,Res,Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.,6
828,EF,"Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner.",1
829,App,"Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests.",2
830,App,"While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time.",3
831,App,"Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter.",4
832,Res,"We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests.",5
833,Res,"We further evaluate on handling “cold start”, and observe consistently better performance by our model when considering various degrees of sparsity of user’s chatting history and conversation contexts.",6
834,EF,"Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.",1
835,App,"In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts.",2
836,EF,"Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context.",3
837,App,"To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations.",4
838,App,"To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions.",5
839,Res,Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.,6
840,EF,Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements.,1
841,EF,"The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction.",2
842,App,"In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding.",3
843,App,The stock embedding is acquired with a deep learning framework using both news articles and price history.,4
844,App,"Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction.",5
845,App,"As one example application, we show the results of portfolio optimization using Reuters & Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data.",6
846,App,This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.,7
847,EF,"Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction.",1
848,EF,"The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically.",2
849,EF,"Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content.",3
850,EF,"This makes it possible to detect likely “fake news” the moment they are published, by simply checking the reliability of their source.",4
851,EF,"From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context.",5
852,App,"Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium’s audience on social media).",6
853,App,We further study (iii) what was written about the target medium (in Wikipedia).,7
854,Res,"The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",8
855,App,We explore the utilities of explicit negative examples in training neural language models.,1
856,EF,"Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks.",2
857,EF,"Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement.",3
858,App,"In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity.",4
859,App,The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word.,5
860,App,We then provide a detailed analysis of the trained models.,6
861,App,One of our findings is the difficulty of object-relative clauses for RNNs.,7
862,Res,We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause.,8
863,Res,"Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses.",9
864,Cont,"Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.",10
865,App,"We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors.",1
866,App,"Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data.",2
867,App,We use this approach to facilitate debugging models on downstream applications.,3
868,Res,Results confirm that the performance of all tested models is affected but the degree of impact varies.,4
869,App,"To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors.",5
870,Res,We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions.,6
871,Res,We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context.,7
872,Cont,Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.,8
873,EF,Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks.,1
874,EF,"The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture.",2
875,EF,"However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model.",3
876,App,"For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA).",4
877,Res,"Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.",5
878,EF,Attention has been proven successful in many natural language processing (NLP) tasks.,1
879,EF,"Recently, many researchers started to investigate the interpretability of attention on NLP tasks.",2
880,EF,Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations.,3
881,App,"In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training.",4
882,App,"We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token’s significance.",5
883,Res,"We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.",6
884,EF,Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems.,1
885,App,"To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples.",2
886,App,R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism.,3
887,App,Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector.,4
888,App,"Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple.",5
889,Res,"Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.",6
890,EF,It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.,1
891,EF,"In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem.",2
892,App,"In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones.",3
893,Res,Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.,4
894,EF,"Most Chinese pre-trained models take character as the basic unit and learn representation according to character’s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese.",1
895,App,"Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models.",2
896,App,"Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion.",3
897,App,"As a result, word and character information are explicitly integrated at the fine-tuning procedure.",4
898,Res,"Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.",5
899,EF,"Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling.",1
900,EF,"By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold.",2
901,App,We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them.,3
902,App,"To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching.",4
903,App,"We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy.",5
904,Res,"Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space.",6
905,App,"We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.",7
906,EF,State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution.,1
907,EF,"For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution.",2
908,App,"In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness.",3
909,App,"Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level).",4
910,Res,Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks.,5
911,Cont,"To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.",6
912,EF,Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages.,1
913,EF,"Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages.",2
914,EF,"However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words.",3
915,App,"To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way.",4
916,App,We first build a graph for each language with its vertices representing different words.,5
917,App,Then we extract word cliques from the graphs and map the cliques of two languages.,6
918,App,"Based on that, we induce the initial word translation solution with the central words of the aligned cliques.",7
919,Res,"This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings.",8
920,App,"Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons.",9
921,Res,Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.,10
922,EF,"Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems?fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance.",1
923,App,"Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning.",2
924,App,"Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture.",3
925,App,"We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer.",4
926,Res,The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples.,5
927,App,"We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.",6
928,EF,"The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance.",1
929,EF,"The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance.",2
930,App,"In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models.",3
931,App,We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model.,4
932,App,"The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation.",5
933,Res,Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores.,6
934,Cont,Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.,7
935,EF,"Recently, large-scale datasets have vastly facilitated the development in nearly all domains of Natural Language Processing.",1
936,EF,"However, there is currently no cross-task dataset in NLP, which hinders the development of multi-task learning.",2
937,App,"We propose MATINF, the first jointly labeled large-scale dataset for classification, question answering and summarization.",3
938,App,MATINF contains 1.07 million question-answer pairs with human-labeled categories and user-generated question descriptions.,4
939,App,"Based on such rich information, MATINF is applicable for three major NLP tasks, including classification, question answering, and summarization.",5
940,App,We benchmark existing methods and a novel multi-task baseline over MATINF to inspire further research.,6
941,Res,Our comprehensive comparison and experiments over MATINF and other datasets demonstrate the merits held by MATINF.,7
942,EF,News recommendation is an important technique for personalized news service.,1
943,EF,"Compared with product and movie recommendations which have been comprehensively studied, the research on news recommendation is much more limited, mainly due to the lack of a high-quality benchmark dataset.",2
944,App,"In this paper, we present a large-scale dataset named MIND for news recommendation.",3
945,App,"Constructed from the user click logs of Microsoft News, MIND contains 1 million users and more than 160k English news articles, each of which has rich textual content such as title, abstract and body.",4
946,App,We demonstrate MIND a good testbed for news recommendation through a comparative study of several state-of-the-art news recommendation methods which are originally developed on different proprietary datasets.,5
947,Res,Our results show the performance of news recommendation highly relies on the quality of news content understanding and user interest modeling.,6
948,Res,Many natural language processing techniques such as effective text representation methods and pre-trained language models can effectively improve the performance of news recommendation.,7
949,Cont,The MIND dataset will be available at https://msnews.github.io.,8
950,EF,"The recent proliferation of ”fake news” has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives.",1
951,EF,"As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again.",2
952,EF,"As manual fact-checking is very time-consuming (and fully automatic fact-checking has credibility issues), it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked.",3
953,EF,"Interestingly, despite the importance of the task, it has been largely ignored by the research community so far.",4
954,App,"Here, we aim to bridge this gap.",5
955,App,"In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work.",6
956,Cont,"We further create a specialized dataset, which we release to the research community.",7
957,App,"Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.",8
958,EF,Open-domain dialogue generation has gained increasing attention in Natural Language Processing.,1
959,EF,Its evaluation requires a holistic means.,2
960,EF,Human ratings are deemed as the gold standard.,3
961,EF,"As human evaluation is inefficient and costly, an automated substitute is highly desirable.",4
962,App,"In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues.",5
963,App,"Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency.",6
964,Res,The empirical validity of our metrics is demonstrated by strong correlations with human judgments.,7
965,Cont,We open source the code and relevant materials.,8
966,EF,The hypernymy detection task has been addressed under various frameworks.,1
967,EF,"Previously, the design of unsupervised hypernymy scores has been extensively studied.",2
968,EF,"In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from “lexical memorization”.",3
969,App,"In this work, we revisit supervised distributional models for hypernymy detection.",4
970,App,"Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE).",5
971,App,"In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations.",6
972,App,"A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections.",7
973,Res,Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.,8
974,EF,Biomedical named entities often play important roles in many biomedical text mining tools.,1
975,EF,"However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging.",2
976,App,"In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities.",3
977,App,"To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates.",4
978,App,Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves.,5
979,App,"In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates.",6
980,Res,"On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.",7
981,EF,"Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks.",1
982,EF,"Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios.",2
983,App,This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages.,3
984,App,We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue.,4
985,Res,"Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.",5
986,EF,This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space.,1
987,App,"To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model.",2
988,App,"Specifically, we considered two types of word classes ? the semantic class of direct objects of a verb and the semantic class in a thesaurus ? and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class.",3
989,Res,"Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution.",4
990,Res,"We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",5
991,EF,"In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC.",1
992,App,"In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency).",2
993,App,"On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation.",3
994,App,"Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference.",4
995,Res,Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines.,5
996,Res,This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.,6
997,EF,The current aspect extraction methods suffer from boundary errors.,1
998,EF,"In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth.",2
999,EF,"However, they hurt the performance severely.",3
1000,App,"In this paper, we propose to utilize a pointer network for repositioning the boundaries.",4
1001,App,"Recycling mechanism is used, which enables the training data to be collected without manual intervention.",5
1002,App,We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant.,6
1003,Res,"Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.",7
1004,EF,"Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification.",1
1005,EF,Most existing studies focused on one of these subtasks only.,2
1006,EF,Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework.,3
1007,EF,"However, the interactive relations among three subtasks are still under-exploited.",4
1008,EF,We argue that such relations encode collaborative signals between different subtasks.,5
1009,EF,"For example, when the opinion term is “delicious”, the aspect term must be “food” rather than “place”.",6
1010,App,"In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network.",7
1011,Res,Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task,8
1012,App,"We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics.",1
1013,App,The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition.,2
1014,Res,Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification.,3
1015,Res,"We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks.",4
1016,App,"Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT.",5
1017,Res,We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.,6
1018,EF,Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text.,1
1019,EF,"Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation.",2
1020,App,"Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction.",3
1021,App,"The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently.",4
1022,Res,"Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.",5
1023,EF,"Previous studies in multimodal sentiment analysis have used limited datasets, which only contain unified multimodal annotations.",1
1024,EF,"However, the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities.",2
1025,App,"In this paper, we introduce a Chinese single- and multi-modal sentiment analysis dataset, CH-SIMS, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations.",3
1026,App,It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis.,4
1027,App,"Furthermore, we propose a multi-task learning framework based on late fusion as the baseline.",5
1028,Res,Extensive experiments on the CH-SIMS show that our methods achieve state-of-the-art performance and learn more distinctive unimodal representations.,6
1029,Cont,The full dataset and codes are available for use at https://github.com/thuiar/MMSA.,7
1030,EF,"End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously.",1
1031,EF,"To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features.",2
1032,EF,"However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered.",3
1033,App,"Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages.",4
1034,App,The difficulty of these courses is gradually increasing.,5
1035,Res,Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.,6
1036,App,"In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system.",1
1037,App,"We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents.",2
1038,App,"We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers.",3
1039,Res,We find different accents exhibiting similar trends irrespective of the probing technique used.,4
1040,Res,"We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.",5
1041,EF,Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts.,1
1042,EF,"Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant.",2
1043,Res,"However, we show that self-training ? a semi-supervised technique for incorporating unlabeled data ? sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations.",3
1044,Res,We also show that ensembling self-trained parsers provides further gains for disfluency detection.,4
1045,EF,Pre-trained language models have achieved huge improvement on many NLP tasks.,1
1046,EF,"However, these methods are usually designed for written text, so they do not consider the properties of spoken language.",2
1047,App,"Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems.",3
1048,App,We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks.,4
1049,Res,The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency.,5
1050,Res,Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.,6
1051,Cont,The code is available at https://github.com/MiuLab/Lattice-ELMo.,7
1052,EF,An increasing number of people in the world today speak a mixed-language as a result of being multilingual.,1
1053,EF,"However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data.",2
1054,App,"We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets.",3
1055,App,"Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data.",4
1056,Res,"Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.",5
1057,EF,Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means.,1
1058,EF,"With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms.",2
1059,EF,"In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions.",3
1060,EF,Thus traditional text-based methods are insufficient to detect multimodal sarcasm.,4
1061,App,"To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context.",5
1062,App,Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net).,6
1063,App,"The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context.",7
1064,Res,Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.,8
1065,App,"In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently.",1
1066,App,"SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation.",2
1067,App,SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)).,3
1068,App,We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech.,4
1069,Res,"Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.",5
1070,EF,"Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR).",1
1071,App,"We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding.",2
1072,App,Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features.,3
1073,App,This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec.,4
1074,App,Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels.,5
1075,Res,We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.,6
1076,EF,Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context.,1
1077,App,"In this paper, we model users’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user’s historical tweet sequence and tweets posted by their neighbours.",2
1078,App,We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context.,3
1079,Res,Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.,4
1080,EF,"Trust is implicit in many online text conversations?striking up new friendships, or asking for tech support.",1
1081,EF,But trust can be betrayed through deception.,2
1082,App,"We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other.",3
1083,App,"Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness.",4
1084,App,"Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives.",5
1085,Res,A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.,6
1086,EF,Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks.,1
1087,App,"In this paper, we present new GFMN formulations that are effective for sequential data.",2
1088,Res,"Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer.",3
1089,App,SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.,4
1090,EF,A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures.,1
1091,EF,"In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks.",2
1092,App,"In this work, we show that this is also the case for text generation from structured and unstructured data.",3
1093,App,"We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively.",4
1094,App,"Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models.",5
1095,Res,Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks.,6
1096,Cont,Code is available at https://github.com/h-shahidi/2birds-gen.,7
1097,App,This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm.,1
1098,App,BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors.,2
1099,App,"By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations.",3
1100,Res,Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors.,4
1101,Res,"Finally, we further show that BHWR produces better representations for rare words.",5
1102,EF,Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.,1
1103,EF,Most of the existing approaches rely on a randomly initialized classifier on top of such networks.,2
1104,EF,"We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.",3
1105,App,"In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.",4
1106,App,"We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.",5
1107,App,"By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches.",6
1108,Res,"Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",7
1109,EF,"In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering.",1
1110,EF,"However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory.",2
1111,App,"To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity.",3
1112,App,Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations.,4
1113,Cont,"It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases.",5
1114,Res,"Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",6
1115,Cont,Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.,7
1116,EF,"Machine translation (MT) has benefited from using synthetic training data originating from translating monolingual corpora, a technique known as backtranslation.",1
1117,EF,Combining backtranslated data from different sources has led to better results than when using such data in isolation.,2
1118,App,"In this work we analyse the impact that data translated with rule-based, phrase-based statistical and neural MT systems has on new MT systems.",3
1119,App,We use a real-world low-resource use-case (Basque-to-Spanish in the clinical domain) as well as a high-resource language pair (German-to-English) to test different scenarios with backtranslation and employ data selection to optimise the synthetic corpora.,4
1120,App,"We exploit different data selection strategies in order to reduce the amount of data used, while at the same time maintaining high-quality MT systems.",5
1121,App,We further tune the data selection method by taking into account the quality of the MT systems used for backtranslation and lexical diversity of the resulting corpora.,6
1122,Res,"Our experiments show that incorporating backtranslated data from different sources can be beneficial, and that availing of data selection can yield improved performance.",7
1123,EF,Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs.,1
1124,EF,This is relevant both for time-critical and on-device computations using neural networks.,2
1125,EF,"The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model.",3
1126,App,"On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity.",4
1127,EF,"Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor.",1
1128,EF,"The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence.",2
1129,EF,"Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.",3
1130,App,"To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process.",4
1131,App,"At each iteration, a base MRC model is trained with golden answers and noisy evidence labels.",5
1132,App,The trained model will predict pseudo evidence labels as extra supervision in the next iteration.,6
1133,App,We evaluate STM on seven datasets over three MRC tasks.,7
1134,Res,"Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.",8
1135,EF,"While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well.",1
1136,EF,This results in poor quantity representations and incorrect solution expressions.,2
1137,App,"In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions.",3
1138,App,"Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs.",4
1139,App,We conduct extensive experiments on two available datasets.,5
1140,Res,Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly.,6
1141,App,We also discuss case studies and empirically examine Graph2Tree’s effectiveness in translating the MWP text into solution expressions.,7
1142,EF,"In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as “positive”, “neutral”, “negative” in sentiment analysis.",1
1143,EF,"Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes).",2
1144,App,"In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory.",3
1145,Res,Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously.,4
1146,Res,"In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.",5
1147,EF,Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks.,1
1148,EF,"However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices).",2
1149,App,"In this paper, we propose a novel method to adaptively compress word embeddings.",3
1150,App,"We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4).",4
1151,App,"However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks.",5
1152,App,The proposed method works in two steps.,6
1153,App,"First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks.",7
1154,App,"After selecting the code length, each word learns discrete codes through a neural network with a binary constraint.",8
1155,App,"To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks.",9
1156,Res,Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy.,10
1157,Res,"Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.",11
1158,App,This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations.,1
1159,App,"We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics.",2
1160,Res,We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements.,3
1161,Res,Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena.,4
1162,Cont,We expect our work to inspire further research in this direction.,5
1163,EF,"Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global).",1
1164,EF,Existing representation learning models do not fully capture these features.,2
1165,App,"To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph.",3
1166,App,The graph is constructed with topical keywords as nodes and multiple local and global features as edges.,4
1167,App,A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them.,5
1168,App,"Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes.",6
1169,Res,"Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.",7
1170,EF,"Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector.",1
1171,EF,"However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge.",2
1172,App,"In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference.",3
1173,Res,"This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.",4
1174,EF,Pretraining deep language models has led to large performance gains in NLP.,1
1175,EF,"Despite this success, Schick and Sch?tze (2020) recently showed that these models struggle to understand rare words.",2
1176,EF,"For static word embeddings, this problem has been addressed by separately learning representations for rare words.",3
1177,App,"In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models.",4
1178,App,This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture.,5
1179,Res,Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.,6
1180,EF,Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly.,1
1181,EF,"However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary.",2
1182,App,"To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences.",3
1183,Res,Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches.,4
1184,Res,"When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models.",5
1185,Res,"Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks.",6
1186,Cont,We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.,7
1187,EF,Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data.,1
1188,EF,It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain.,2
1189,App,"In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation.",3
1190,EF,"Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge.",4
1191,App,"To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task.",5
1192,App,The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way.,6
1193,App,"Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features.",7
1194,Res,Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin.,8
1195,Res,The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.,9
1196,EF,Generating a concise summary from a large collection of arguments on a given topic is an intriguing yet understudied problem.,1
1197,App,"We propose to represent such summaries as a small set of talking points, termed key points, each scored according to its salience.",2
1198,Res,"We show, by analyzing a large dataset of crowd-contributed arguments, that a small number of key points per topic is typically sufficient for covering the vast majority of the arguments.",3
1199,Res,"Furthermore, we found that a domain expert can often predict these key points in advance.",4
1200,App,"We study the task of argument-to-key point mapping, and introduce a novel large-scale dataset for this task.",5
1201,Res,"We report empirical results for an extensive set of experiments with this dataset, showing promising performance.",6
1202,EF,"Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior.",1
1203,EF,"Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks.",2
1204,App,"We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral.",3
1205,App,We demonstrate the high quality of the annotations via Principal Preserved Component Analysis.,4
1206,App,We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies.,5
1207,Res,"Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement.",6
1208,EF,"In a context of offensive content mediation on social media now regulated by European laws, it is important not only to be able to automatically detect sexist content but also to identify if a message with a sexist content is really sexist or is a story of sexism experienced by a woman.",1
1209,App,"We propose: (1) a new characterization of sexist content inspired by speech acts theory and discourse analysis studies, (2) the first French dataset annotated for sexism detection, and (3) a set of deep learning experiments trained on top of a combination of several tweet’s vectorial representations (word embeddings, linguistic features, and various generalization strategies).",2
1210,Cont,Our results are encouraging and constitute a first step towards offensive content moderation.,3
1211,EF,"Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches.",1
1212,EF,"However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches.",2
1213,App,"In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks.",3
1214,App,"With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation.",4
1215,App,"In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair.",5
1216,Res,"Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets.",6
1217,Cont,We release our code at https://github.com/baidu/Senta.,7
1218,EF,Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces.,1
1219,EF,"However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism.",2
1220,App,"In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages.",3
1221,App,"We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure.",4
1222,Res,We find that both models exhibit a preference for UD over SUD ? with interesting variations across languages and layers ? and that the strength of this preference is correlated with differences in tree shape.,5
1223,EF,Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences.,1
1224,EF,"Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date.",2
1225,App,"In this paper, we show that these results can be improved by using an in-order linearization instead.",3
1226,App,"Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al.",4
1227,EF,"(2015)’s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers.",5
1228,Res,"Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.",6
1229,EF,"A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar.",1
1230,Res,"We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs).",2
1231,Res,The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules.,3
1232,App,"In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.",4
1233,EF,Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT.,1
1234,EF,"Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like.",2
1235,EF,A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank.,3
1236,App,"We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias.",4
1237,Res,"While known techniques for tackling these biases improve results, they still do not make the parser state of the art.",5
1238,App,"Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation.",6
1239,Res,"The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.",7
1240,EF,Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser.,1
1241,EF,"However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology.",2
1242,App,"In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech.",3
1243,App,"In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs).",4
1244,Res,We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech.,5
1245,App,We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.,6
1246,EF,"With the recent proliferation of the use of text classifications, researchers have found that there are certain unintended biases in text classification datasets.",1
1247,EF,"For example, texts containing some demographic identity-terms (e.g., “gay”, “black”) are more likely to be abusive in existing abusive language detection datasets.",2
1248,EF,"As a result, models trained with these datasets may consider sentences like “She makes me happy to be gay” as abusive simply because of the word “gay.”",3
1249,App,"In this paper, we formalize the unintended biases in text classification datasets as a kind of selection bias from the non-discrimination distribution to the discrimination distribution.",4
1250,App,"Based on this formalization, we further propose a model-agnostic debiasing training framework by recovering the non-discrimination distribution using instance weighting, which does not require any extra resources or annotations apart from a pre-defined set of demographic identity-terms.",5
1251,Res,Experiments demonstrate that our method can effectively alleviate the impacts of the unintended biases without significantly hurting models’ generalization ability.,6
1252,EF,"Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method.",1
1253,App,As a step in this direction we study the case of representations of phonology in neural network models of spoken language.,2
1254,App,"We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences.",3
1255,App,We manipulate two factors that can affect the outcome of analysis.,4
1256,App,"First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models.",5
1257,App,"Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance.",6
1258,Cont,"We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.",7
1259,EF,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.",1
1260,App,"In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations.",2
1261,App,We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.,3
1262,App,"Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.",4
1263,App,"Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.",5
1264,Res,Our framework shows that this model is capable of generating a significant number of inconsistent explanations.,6
1265,EF,"By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings).",1
1266,EF,The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.,2
1267,EF,"However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.",3
1268,App,"Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).",4
1269,App,"Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.",5
1270,Res,Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.,6
1271,Res,We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.,7
1272,EF,"Language models keep track of complex information about the preceding context ? including, e.g., syntactic relations in a sentence.",1
1273,App,We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.,2
1274,App,"We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.",3
1275,Res,The Transformer outperforms the LSTM in all analyses.,4
1276,Res,"Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.",5
1277,Res,"However, we find traces of the latter aspect, too.",6
1278,EF,"In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer.",1
1279,EF,"Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.",2
1280,EF,This makes attention weights unreliable as explanations probes.,3
1281,App,"In this paper, we consider the problem of quantifying this flow of information through self-attention.",4
1282,App,"We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens.",5
1283,Res,"We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",6
1284,EF,"With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems.",1
1285,EF,"But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research.",2
1286,App,"We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria.",3
1287,App,"We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community.",4
1288,App,We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted.,5
1289,App,"Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful.",6
1290,App,"We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",7
1291,EF,Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions.,1
1292,EF,Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction.,2
1293,EF,They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions.,3
1294,App,"In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions.",4
1295,EF,We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions.,5
1296,EF,"Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions.",6
1297,App,"To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse.",7
1298,Res,We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods.,8
1299,Res,Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions.,9
1300,Cont,Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention,10
1301,EF,Multi-task Learning methods have achieved great progress in text classification.,1
1302,EF,"However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications.",2
1303,App,"To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption.",3
1304,Res,The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.,4
1305,App,"This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.",1
1306,App,Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.,2
1307,Res,"The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.",3
1308,EF,Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training.,1
1309,EF,"However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading.",2
1310,App,"In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances).",3
1311,App,"We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills.",4
1312,Res,We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.,5
1313,App,"This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).",1
1314,App,The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC.,2
1315,App,"For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods.",3
1316,Res,"Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM.",4
1317,Res,The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks.,5
1318,Cont,Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.,6
1319,EF,"With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents.",1
1320,EF,Most existing methods usually learn the representations of users and news from news contents for recommendation.,2
1321,EF,"However, they seldom consider high-order connectivity underlying the user-news interactions.",3
1322,EF,"Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news.",4
1323,App,"In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD.",5
1324,App,Our model can encode high-order relationships into user and news representations by information propagation along the graph.,6
1325,App,"Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability.",7
1326,App,"A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations.",8
1327,Res,Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.,9
1328,App,"In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case.",1
1329,App,We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story.,2
1330,App,"We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework.",3
1331,Res,Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.,4
1332,EF,"The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online.",1
1333,EF,"Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection.",2
1334,EF,"While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language.",3
1335,EF,"The latter is, however, inextricably linked to abusive behaviour.",4
1336,App,"In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other.",5
1337,Res,Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.,6
1338,EF,The key to effortless end-user programming is natural language.,1
1339,App,"We examine how to teach intelligent systems new functions, expressed in natural language.",2
1340,App,"As a first step, we collected 3168 samples of teaching efforts in plain English.",3
1341,App,"Then we built fuSE, a novel system that translates English function descriptions into code.",4
1342,App,Our approach is three-tiered and each task is evaluated separately.,5
1343,App,We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT).,6
1344,App,Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM).,7
1345,App,"Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods).",8
1346,Res,In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.,9
1347,EF,Moderation is crucial to promoting healthy online discussions.,1
1348,EF,"Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.",2
1349,App,"We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems?",3
1350,App,"We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title.",4
1351,Res,We find that context can both amplify or mitigate the perceived toxicity of posts.,5
1352,Res,"Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.",6
1353,Res,"Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.",7
1354,Res,This points to the need for larger datasets of comments annotated in context.,8
1355,Cont,We make our code and data publicly available.,9
1356,EF,Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences.,1
1357,App,We investigate parsing AMR with explicit dependency structures and interpretable latent structures.,2
1358,App,"We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks.",3
1359,Res,The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).,4
1360,EF,Answering natural language questions over tables is usually seen as a semantic parsing task.,1
1361,EF,"To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.",2
1362,EF,"However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.",3
1363,App,"In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.",4
1364,App,"TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.",5
1365,App,"TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.",6
1366,Res,"We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture.",7
1367,Res,"We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",8
1368,EF,"In argumentation, people state premises to reason towards a conclusion.",1
1369,EF,"The conclusion conveys a stance towards some target, such as a concept or statement.",2
1370,EF,"Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons.",3
1371,EF,"However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation.",4
1372,App,We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises.,5
1373,App,"In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets.",6
1374,App,"We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network.",7
1375,Res,"Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines.",8
1376,Res,"According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.",9
1377,EF,"Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality.",1
1378,EF,"Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities.",2
1379,EF,Equally treating all modalities may encode too much useless information from less important modalities.,3
1380,App,"In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.",4
1381,App,"The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images.",5
1382,Res,Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.,6
